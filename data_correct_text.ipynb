{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Working on:  C:\\Users\\adity\\OneDrive\\NCBS\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "%matplotlib widget\n",
    "\n",
    "from eidynamics import ephys_classes as ephys\n",
    "from eidynamics import utils, plot_tools\n",
    "from eidynamics import ephys_functions\n",
    "import all_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load any cell and check if the data matches the raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the 5th cell from all_cells\n",
    "cellpath = all_cells.project_path_root / all_cells.all_cells[25]\n",
    "cellname = cellpath.stem\n",
    "cellfile =  cellpath / (cellname + '.pkl')\n",
    "# cellpath = all_cells.project_path_root / cellname / \n",
    "print(cellname, cellfile)\n",
    "cell = ephys.Neuron.loadCell(cellfile)\n",
    "data = cell.data['SpikeTrain']\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fq20 = data#[data['stimFreq']==20]\n",
    "\n",
    "# plot all traces from columns 36:20036 from every row and add a text from column 'AP' next to the trace\n",
    "plt.figure()\n",
    "time = np.linspace(0,10,200000)\n",
    "for i in range(fq20.shape[0]):\n",
    "\n",
    "    plt.plot(time+0.05*i, 10*i+ fq20.iloc[i,36:200036])\n",
    "    texttoshow = fq20.iloc[i,:]['AP']\n",
    "    plt.text(0, 10*i, texttoshow)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.metadata_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldfs = [\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_SpikeTrain_combined_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_SpikeTrain_VC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_convergence_VC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_convergence_CC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_convergence_combined_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_convergence_CC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_convergence_combined_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_convergence_VC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_surprise_VC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_surprise_CC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_surprise_combined_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_surprise_CC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_surprise_combined_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_surprise_VC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_LTMRand_CC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_LTMRand_VC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_LTMRand_combined_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_LTMRand_CC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_LTMRand_combined_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_LTMRand_VC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_grid_VC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_grid_CC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_grid_combined_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_grid_CC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_grid_VC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_grid_combined_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_FreqSweep_VC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_FreqSweep_VC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_FreqSweep_combined_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_FreqSweep_CC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_FreqSweep_CC_short.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_FreqSweep_combined_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_SpikeTrain_VC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_SpikeTrain_CC_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_SpikeTrain_combined_long.h5\",\n",
    "\"C:\\\\Users\\\\adity\\\\OneDrive\\\\NCBS\\\\Lab\\\\Projects\\\\EI_Dynamics\\\\Analysis\\\\parsed_data\\\\all_cells_SpikeTrain_CC_short.h5\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1. Load each dataframe and get the 'sweepLength' and 'clampMode' parameters from the first row\n",
    "## 2. multiply sweeplength by 2e4 to get the number of samples, this will be datalength\n",
    "## 3. from each row, check if thew data stored in columns from 49 to 49+datalength crosses the threshold. Threshold is given by spikeThreshold={'CC':0, 'VC':1000, 'Loose':0}\n",
    "## 4. if the maxvalue for the rowdata is above the threshold, then change the value of the 'AP' column to 1, else 0\n",
    "## 5. save the dataframe as a .h5 file in the folder 'parsed_data'\n",
    "\n",
    "# all dataframes are stored as .h5 file in the folder 'parsed_data', get the list and go through each file as described above\n",
    "\n",
    "spikeThreshold={'CC':70, 'VC':1000, 'Loose':70}\n",
    "\n",
    "# list of all the df files is in the variable alldfs\n",
    "for dfpath in alldfs:\n",
    "    dfpath = pathlib.Path(dfpath)\n",
    "    # if the filepath has \"short\" suffix in it, then move to next iteration\n",
    "    if 'short' in dfpath.stem:\n",
    "        print('skipping: ', dfpath)\n",
    "        continue\n",
    "    df = pd.read_hdf(dfpath)\n",
    "    print('processing: ', dfpath, df.shape)\n",
    "    if df.shape[0] == 0:\n",
    "        print('No data in: ', dfpath, '   Skipping')\n",
    "        continue\n",
    "    datalength = int(df.iloc[0]['sweepLength']*2e4)\n",
    "    # Create a boolean Series where each element is True if the max of the row slice is greater than the spikeThreshold\n",
    "    is_spike = df.iloc[:, 49:49+datalength].max(axis=1) > df['clampMode'].map(spikeThreshold)\n",
    "    # Convert the boolean Series to int (True becomes 1 and False becomes 0)\n",
    "    df['AP'] = is_spike.astype(int)\n",
    "    print(\"-------> saving: \", dfpath)\n",
    "    df.to_hdf(dfpath, key='data', mode='w')\n",
    "    # make a short dataframe and save it as a .h5 file\n",
    "    shortdf_filename = dfpath.parent / dfpath.name.replace('long', 'short')\n",
    "    print('-------> saving: ', shortdf_filename)\n",
    "    #save first 49 columns of df into df_short\n",
    "    df_short = df.iloc[:,:49]\n",
    "    # save df_short as a .h5 file\n",
    "    df_short.to_hdf(shortdf_filename, key='data', mode='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(columns=['protocol', 'clamp','cellID', 'sweeps'])\n",
    "allentries = []\n",
    "for shortdf in allshortdfs:\n",
    "    dfpath = pathlib.Path(shortdf)\n",
    "    # if the filepath has combined suffix in it, don't process\n",
    "    if 'combined' in dfpath.stem:\n",
    "        print('skipping: ', dfpath)\n",
    "        continue\n",
    "    df = pd.read_hdf(dfpath)\n",
    "    protocol = dfpath.stem.split('_')[2]\n",
    "    clamp = dfpath.stem.split('_')[3]\n",
    "    cells = df['cellID'].unique()\n",
    "    for c in cells:\n",
    "        sweeps = df[df['cellID']==c].shape[0]\n",
    "        newentry = [protocol, clamp, c, sweeps]\n",
    "        allentries.append(newentry)    \n",
    "# make allentries into a dataframe\n",
    "df2 = pd.DataFrame(allentries, columns=['protocol', 'clamp', 'cellID', 'sweeps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(r\"C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\allcells_and_sweepnumbers_across_protocols_16jan2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### combine and cut dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_convergence_combined_long.h5\n",
      "(508, 200049) 50000\n",
      "0 12049 100049 112049 150049 162049\n",
      "cut df has a shape:  (508, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_convergence_combined_cut_600ms.h5\n",
      "processing:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_surprise_combined_long.h5\n",
      "(480, 400049) 100000\n",
      "0 12049 200049 212049 300049 312049\n",
      "cut df has a shape:  (480, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_surprise_combined_cut_600ms.h5\n",
      "processing:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_LTMRand_combined_long.h5\n",
      "(552, 80049) 20000\n",
      "0 12049 40049 52049 60049 72049\n",
      "cut df has a shape:  (552, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_LTMRand_combined_cut_600ms.h5\n",
      "processing:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_grid_combined_long.h5\n",
      "(5760, 80049) 20000\n",
      "0 12049 40049 52049 60049 72049\n",
      "cut df has a shape:  (5760, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_grid_combined_cut_600ms.h5\n",
      "processing:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_SpikeTrain_combined_long.h5\n",
      "(192, 880049) 220000\n",
      "0 12049 440049 452049 660049 672049\n",
      "cut df has a shape:  (192, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_SpikeTrain_combined_cut_600ms.h5\n",
      "combined part1 df1 has a shape:  (7492, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_allprotocols_4channels_combined_600ms_part1.h5\n",
      "processing:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_FreqSweep_VC_long.h5\n",
      "(4407, 80049) 20000\n",
      "0 12049 40049 52049 60049 72049\n",
      "cut df has a shape:  (4407, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_FreqSweep_VC_cut_600ms.h5\n",
      "processing:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_FreqSweep_CC_long.h5\n",
      "(4971, 80049) 20000\n",
      "0 12049 40049 52049 60049 72049\n",
      "cut df has a shape:  (4971, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_FreqSweep_CC_cut_600ms.h5\n",
      "combined part2 df2 has a shape:  (9378, 36049)\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_allprotocols_4channels_combined_600ms_part1.h5\n",
      "saving:  C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_allprotocols_4channels_combined_600ms.h5\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "Fs = 2e4\n",
    "cuttime = 0.600\n",
    "datapoints = int(cuttime*Fs)\n",
    "timetxt = str(cuttime*1000).replace('.', 'ms')[:-1]\n",
    "\n",
    "newdfs = []\n",
    "for dfpath in alldfs:\n",
    "    dfpath = pathlib.Path(dfpath)\n",
    "    # if the filepath has \"short\" suffix in it, then move to next iteration\n",
    "    if ('combined_long' in dfpath.stem) & ('FreqSweep' not in dfpath.stem):\n",
    "        print('processing: ', dfpath)\n",
    "        df = pd.read_hdf(dfpath)\n",
    "        sweeplength = int(df.iloc[0]['sweepLength']*Fs)\n",
    "        print(df.shape, sweeplength)\n",
    "        t1,t2 = 0, 49+datapoints # 0, 7049\n",
    "        t3,t4 = 49+2*sweeplength, 49+2*sweeplength+datapoints # 40049, 47049\n",
    "        t5,t6 = 49+3*sweeplength, 49+3*sweeplength+datapoints # 60049, 67049\n",
    "        print(t1,t2,t3,t4,t5,t6)\n",
    "        dfcut = pd.concat([df.iloc[:,:t2], df.iloc[:,t3:t4], df.iloc[:,t5:t6],], axis=1)\n",
    "        \n",
    "        # rename the columns of dfcut from 49 till the end as 0:21000, preserve the column names of first 49 columns\n",
    "        dfcut.columns = list(df.columns[:49]) + list(range(dfcut.shape[1]-49))\n",
    "        print(\"cut df has a shape: \", dfcut.shape)\n",
    "        del df\n",
    "        \n",
    "        # save dfcut as a .h5 file\n",
    "        newname = dfpath.parent / dfpath.name.replace('combined_long', 'combined_cut_'+timetxt)\n",
    "        print('saving: ', newname)\n",
    "        # suppress performance warning for the following line:\n",
    "\n",
    "        dfcut.to_hdf(newname, key='data', mode='w')\n",
    "        # add dfcut to newdfs\n",
    "        newdfs.append(dfcut)\n",
    "\n",
    "df1 = pd.concat(newdfs, axis=0)\n",
    "print(\"combined part1 df1 has a shape: \", df1.shape)\n",
    "# save df\n",
    "newfile1 = r\"C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_allprotocols_4channels_combined_\"+timetxt+\"_part1.h5\"\n",
    "print('saving: ', newfile1)\n",
    "df1.to_hdf(newfile1, key='data', mode='w')\n",
    "del newdfs, dfcut\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "newdfs = []\n",
    "for dfpath in alldfs:\n",
    "    dfpath = pathlib.Path(dfpath)\n",
    "    # if the filepath has \"short\" suffix in it, then move to next iteration\n",
    "    if ('_long' in dfpath.stem) & ('FreqSweep' in dfpath.stem) & ('combined' not in dfpath.stem):\n",
    "        print('processing: ', dfpath)\n",
    "        df = pd.read_hdf(dfpath)\n",
    "        sweeplength = int(df.iloc[0]['sweepLength']*Fs)\n",
    "        print(df.shape, sweeplength)\n",
    "        t1,t2 = 0, 49+datapoints # 0, 7049\n",
    "        t3,t4 = 49+2*sweeplength, 49+2*sweeplength+datapoints # 40049, 47049\n",
    "        t5,t6 = 49+3*sweeplength, 49+3*sweeplength+datapoints # 60049, 67049\n",
    "        print(t1,t2,t3,t4,t5,t6)\n",
    "        dfcut = pd.concat([df.iloc[:,:t2], df.iloc[:,t3:t4], df.iloc[:,t5:t6],], axis=1)\n",
    "        \n",
    "        # rename the columns of dfcut from 49 till the end as 0:21000, preserve the column names of first 49 columns\n",
    "        dfcut.columns = list(df.columns[:49]) + list(range(dfcut.shape[1]-49))\n",
    "        print(\"cut df has a shape: \", dfcut.shape)\n",
    "        del df\n",
    "        \n",
    "        # save dfcut as a .h5 file\n",
    "        newname = dfpath.parent / dfpath.name.replace('_long', '_cut_'+timetxt)\n",
    "        print('saving: ', newname)\n",
    "        # suppress performance warning for the following line:\n",
    "\n",
    "        dfcut.to_hdf(newname, key='data', mode='w')\n",
    "        # add dfcut to newdfs\n",
    "        newdfs.append(dfcut)\n",
    "\n",
    "\n",
    "df2 = pd.concat(newdfs, axis=0)\n",
    "print(\"combined part2 df2 has a shape: \", df2.shape)\n",
    "# save df\n",
    "newfile2 = r\"C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_allprotocols_4channels_combined_\"+timetxt+\"_part2.h5\"\n",
    "print('saving: ', newfile2)\n",
    "df2.to_hdf(newfile2, key='data', mode='w')\n",
    "del newdfs, dfcut\n",
    "\n",
    "\n",
    "# #-----------------------------------------------------------\n",
    "# join the two dfs\n",
    "df = pd.concat([df1, df2], axis=0)\n",
    "finalfile = r\"C:\\Users\\adity\\OneDrive\\NCBS\\Lab\\Projects\\EI_Dynamics\\Analysis\\parsed_data\\all_cells_allprotocols_4channels_combined_\"+timetxt+\".h5\"\n",
    "print('saving: ', finalfile)\n",
    "df.to_hdf(finalfile, key='data', mode='w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-ephys",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
