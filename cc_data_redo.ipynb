{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from   pathlib      import Path\n",
    "# import importlib\n",
    "\n",
    "# import numpy                as np\n",
    "# import matplotlib           as mpl\n",
    "# import matplotlib.pyplot    as plt\n",
    "# import seaborn              as sns\n",
    "# import pandas               as pd\n",
    "\n",
    "# from scipy.stats   import kruskal, wilcoxon, mannwhitneyu, ranksums\n",
    "# from scipy.optimize import curve_fit\n",
    "# import statsmodels.api as sm\n",
    "# from statsmodels.multivariate.manova import MANOVA\n",
    "# import statsmodels.formula.api as smf\n",
    "# from eidynamics     import utils, plot_tools\n",
    "# plt.rcParams['font.family'] = 'Arial'\n",
    "# plt.rcParams['font.size'] = 12\n",
    "# plt.rcParams['svg.fonttype'] = 'none'\n",
    "\n",
    "# # make a colour map viridis\n",
    "# viridis = mpl.colormaps[\"viridis\"]\n",
    "# flare   = mpl.colormaps[\"flare\"]\n",
    "# crest   = mpl.colormaps[\"crest\"]\n",
    "# magma   = mpl.colormaps[\"magma\"]\n",
    "# edge    = mpl.colormaps['edge']\n",
    "\n",
    "# color_E = \"flare\"\n",
    "# color_I = \"crest\"\n",
    "# color_freq = {1:magma(0.05), 5:magma(0.1), 10:magma(0.2), 20:magma(.4), 30:magma(.5), 40:magma(.6), 50:magma(.7), 100:magma(.9)}\n",
    "# color_squares = color_squares = {1:viridis(0.2), 5:viridis(.4), 7:viridis(.6), 15:viridis(.8), 20:viridis(1.0)}\n",
    "# color_EI = {-70:flare(0), 0:crest(0)}\n",
    "# colors_EI = {-70:flare, 0:crest}\n",
    "\n",
    "# Fs = 2e4\n",
    "# %matplotlib widget\n",
    "# freq_sweep_pulses = np.arange(9)\n",
    "\n",
    "# from eidynamics.fit_PSC     import find_sweep_expected\n",
    "# # from Findsim        import tab_presyn_patterns_LR_43\n",
    "# # import parse_data\n",
    "# from eidynamics     import utils, plot_tools\n",
    "# import all_cells\n",
    "# # import plotFig2\n",
    "# import stat_annotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import plotFig2\n",
    "%colors nocolor\n",
    "Fs=2e4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "figure_raw_material_location = Path(r\"paper_figure_matter\\\\\")\n",
    "data_path                    = Path(r\"parsed_data\\\\FreqSweep\\\\\")\n",
    "\n",
    "# Load the dataset\n",
    "freq_sweep_cc_datapath =  Path(r\"parsed_data\\\\FreqSweep\\\\all_cells_FreqSweep_CC_long.h5\")\n",
    "df = pd.read_hdf(freq_sweep_cc_datapath, key='data')\n",
    "\n",
    "# # expanded dataframe (processed dataframe with metadata and analysed params)\n",
    "# expanded_data_path = Path(r\"parsed_data\\all_cells_FreqSweep_combined_expanded.h5\")\n",
    "# xc_FS_analyseddf = pd.read_hdf(expanded_data_path, key='data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4971, 80073)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns that are not needed\n",
    "# not needed columns are 20049 to 80049\n",
    "df = df.drop(df.columns[20049:80049], axis=1)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(plotFig2)\n",
    "\n",
    "new_fields = ['stimtime_','valley_', 'peak_', 'slope_', 'peakdelay_', 'onsetdelay_','normpeak_', 'PSC_', 'normPSC_', 'spike_']\n",
    "newcolumns = [field + str(i) for field in new_fields for i in range(9)]\n",
    "Fs=2e4\n",
    "# Adding new columns with NaN values\n",
    "df = df.assign(**{col: np.nan for col in newcolumns})\n",
    "c = 0\n",
    "d=0\n",
    "r=0\n",
    "k=0\n",
    "for idx, row in df.iterrows():\n",
    "    print(f\"Processing {idx} row with trialID {row['trialID']}\")\n",
    "    # do not process if freq==10\n",
    "    if row['stimFreq'] <20:\n",
    "        print(f\"Skipping {idx} row with trialID {row['trialID']}: Low frequency\")\n",
    "        d+=0\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        if row['probePulseStart'] == row['pulseTrainStart']:\n",
    "            freq = row['stimFreq']\n",
    "            isi = int(0.9*Fs / freq)\n",
    "            print(f\"interpolating {idx} row with trialID {row['trialID']}: No probe pulse. {row['probePulseStart']} =/= { row['pulseTrainStart']}\")\n",
    "            row[2000:2000+isi] = row[4620:4620+isi]\n",
    "            row[2000+isi-100:4620] = np.interp(np.arange(2000+isi-100,4620), [2000+isi-100,4620], [row[2000+isi-100], row[4620]])\n",
    "            c+=1\n",
    "            results = plotFig2.deconv(row[49:80049], row['stimFreq'], 0.1, row['pulseTrainStart'], None, noprobepulse=False)\n",
    "        else:\n",
    "            results = plotFig2.deconv(row[49:80049], row['stimFreq'], row['probePulseStart'], row['pulseTrainStart'], None, noprobepulse=False)\n",
    "    except RuntimeError:\n",
    "        r+=1\n",
    "        continue\n",
    "    except FloatingPointError as e:\n",
    "        print(f\"Error at {idx} row with trialID {row['trialID']}: {e}\")\n",
    "        print(row['stimFreq'], row['numSq'], row['probePulseStart'], row['pulseTrainStart'])\n",
    "        k+=1\n",
    "        continue\n",
    "    \n",
    "    valleyTimes, valleys, peakTimes, peaks = results[2]\n",
    "    # elementwise difference between peak and valley is PSC\n",
    "    pscs = np.array(peaks) - np.array(valleys)\n",
    "    normPSCs = np.array(pscs) / pscs[0]\n",
    "    normpeaks = np.array(peaks) / peaks[0]\n",
    "    stimTimes = np.array(results[-1]) / Fs    \n",
    "    slopes = np.array([(peaks[i] - valleys[i]) / (peakTimes[i] - valleyTimes[i]) for i in range(len(peaks))])\n",
    "    peakdelays = np.array([(peakTimes[i] - stimTimes[i]) for i in range(len(valleyTimes))])\n",
    "    onsetdelays = np.array([(valleyTimes[i] - stimTimes[i]) for i in range(len(valleyTimes))])\n",
    "    # spikes if peaks are greater than 20\n",
    "    spikes = np.array([1 if peak > 20 else 0 for peak in peaks])\n",
    "\n",
    "    for col in newcolumns:\n",
    "        prefix, col_idx = col.split('_')\n",
    "        col_idx = int(col_idx)\n",
    "\n",
    "        if col_idx < len(valleys):  # Ensure index is within bounds\n",
    "            if prefix == 'stimtime':\n",
    "                df.loc[row.name, col] = stimTimes[col_idx]\n",
    "            elif prefix == 'valley':\n",
    "                df.loc[row.name, col] = valleys[col_idx]\n",
    "            elif prefix == 'peak':\n",
    "                df.loc[row.name, col] = peaks[col_idx]\n",
    "            elif prefix == 'slope':\n",
    "                df.loc[row.name, col] = slopes[col_idx]\n",
    "            elif prefix == 'peakdelay':\n",
    "                df.loc[row.name, col] = peakdelays[col_idx]\n",
    "            elif prefix == 'onsetdelay':\n",
    "                df.loc[row.name, col] = onsetdelays[col_idx]\n",
    "            elif prefix == 'normpeak':\n",
    "                df.loc[row.name, col] = normpeaks[col_idx]\n",
    "            elif prefix == 'PSC':\n",
    "                df.loc[row.name, col] = pscs[col_idx]\n",
    "            elif prefix == 'normPSC':\n",
    "                df.loc[row.name, col] = normPSCs[col_idx]\n",
    "            elif prefix == 'spike':\n",
    "                df.loc[row.name, col] = spikes[col_idx]\n",
    "\n",
    "\n",
    "    # counter for every 100 rows\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Processed {idx} rows\")\n",
    "\n",
    "# Assuming df is your final DataFrame\n",
    "metadata_cols1 = df.iloc[:, :49]\n",
    "metadata_cols2 = df.iloc[:, 20049:]\n",
    "\n",
    "# Concatenating the two slices into a new DataFrame\n",
    "new_df = pd.concat([metadata_cols1, metadata_cols2], axis=1)\n",
    "\n",
    "# Display the shape of the new DataFrame to verify\n",
    "print(new_df.shape)\n",
    "print(f\"Kernel errors: {k}, Runtime errors: {r}, No probe pulse: {c}, Low frequency: {d}\")\n",
    "4113"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(new_df.shape)\n",
    "# save df\n",
    "new_df.to_hdf(data_path / \"all_cells_FreqSweep_CC_kernelfit_response_measurements.h5\", key='data', mode='w')\n",
    "# save as excel\n",
    "new_df.to_excel(data_path / \"all_cells_FreqSweep_CC_kernelfit_response_measurements.xlsx\")\n",
    "\n",
    "# drop those rows from df that have NaN values in column 'valley_0'\n",
    "new_df = new_df.dropna(subset=['valley_0'])\n",
    "print(new_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save df\n",
    "new_df.to_hdf(data_path / \"all_cells_FreqSweep_CC_kernelfit_response_measurements_noNANs.h5\", key='data', mode='w')\n",
    "# save as excel\n",
    "new_df.to_excel(data_path / \"all_cells_FreqSweep_CC_kernelfit_response_measurements_noNANs.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
